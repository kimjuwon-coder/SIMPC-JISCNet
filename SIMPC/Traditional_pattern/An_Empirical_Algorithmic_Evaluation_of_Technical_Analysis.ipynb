{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztdCu22vaY-P",
        "outputId": "42d51a30-7517-4fe4-a9d4-5b6dbcf534c3"
      },
      "outputs": [],
      "source": [
        "!pip install mplfinance\n",
        "!pip install seaborn\n",
        "!pip install finance-datareader\n",
        "!pip install pandas_datareader "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiXqAGZUaPnz"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
        "from numpy import linspace\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import FinanceDataReader as fdr\n",
        "from scipy.signal import argrelextrema\n",
        "sns.set()\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [16, 9]\n",
        "plt.rcParams['figure.dpi'] = 200\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JibIvdKNaPn3"
      },
      "source": [
        "## 극점 찾기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taNd9OZZaPn4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "identification_lag=4\n",
        "\n",
        "def find_max_min(prices):\n",
        "    prices_ = prices.copy()\n",
        "    prices_.index = linspace(1., len(prices_), len(prices_))\n",
        "    kr = KernelReg([prices_.values], [prices_.index.values], var_type='c',  bw=np.full((1, 1), 0.8))\n",
        "    f = kr.fit([prices_.index.values])\n",
        "    smooth_prices = pd.Series(data=f[0], index=prices.index)\n",
        "\n",
        "    local_max = argrelextrema(smooth_prices.values, np.greater)[0]\n",
        "    local_min = argrelextrema(smooth_prices.values, np.less)[0]\n",
        "\n",
        "    price_local_max_dt = []\n",
        "    for i in local_max:\n",
        "        if (i>1) and (i<(len(prices)-identification_lag-2)):\n",
        "            price_local_max_dt.append(prices.iloc[i-2:i+2].idxmax())\n",
        "\n",
        "    price_local_min_dt = []\n",
        "    for i in local_min:\n",
        "        if (i>1) and (i<(len(prices)-identification_lag-2)):\n",
        "            price_local_min_dt.append(prices.iloc[i-2:i+2].idxmin())\n",
        "\n",
        "    price_local_max_dt = pd.to_datetime(price_local_max_dt)\n",
        "    price_local_min_dt = pd.to_datetime(price_local_min_dt)\n",
        "\n",
        "    prices.name = 'price'\n",
        "    maxima = pd.DataFrame(prices.loc[prices.index.isin(price_local_max_dt)])\n",
        "    minima = pd.DataFrame(prices.loc[prices.index.isin(price_local_min_dt)])\n",
        "    max_min = pd.concat([maxima, minima]).sort_index()\n",
        "    max_min.index.name = 'date'\n",
        "    max_min = max_min.reset_index()\n",
        "    max_min = max_min[~max_min.date.duplicated()]\n",
        "\n",
        "    p = prices.reset_index()\n",
        "    max_min['day_num'] = p[p['index'].isin(max_min.date)].index.values\n",
        "    max_min = max_min.set_index('day_num').price\n",
        "\n",
        "    return max_min, smooth_prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8FFGHB-aPn4"
      },
      "outputs": [],
      "source": [
        "window_size_min = 18\n",
        "window_size_max = 22"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvIdhwD7aPn4"
      },
      "source": [
        "## 규칙에 맞는 패턴 찾기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "j6tTbdX5aPn4"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def find_patterns(max_min):\n",
        "    patterns = defaultdict(list)\n",
        "\n",
        "    for i in range(5, len(max_min)):\n",
        "        window = max_min.iloc[i-5:i]\n",
        "\n",
        "        # pattern must play out in less than 36 days\n",
        "        if (window.index[-1] - window.index[0] > window_size_max) or (window.index[-1] - window.index[0] < window_size_min):\n",
        "            continue\n",
        "\n",
        "        # Using the notation from the paper to avoid mistakes\n",
        "        e1 = window.iloc[0]\n",
        "        e2 = window.iloc[1]\n",
        "        e3 = window.iloc[2]\n",
        "        e4 = window.iloc[3]\n",
        "        e5 = window.iloc[4]\n",
        "\n",
        "        rtop_g1 = np.mean([e1,e3,e5])\n",
        "        rtop_g2 = np.mean([e2,e4])\n",
        "        # Head and Shoulders\n",
        "        if (e1 > e2) and (e3 > e1) and (e3 > e5) and \\\n",
        "            (abs(e1 - e5) <= 0.03*np.mean([e1,e5])) and \\\n",
        "            (abs(e2 - e4) <= 0.03*np.mean([e1,e5])):\n",
        "                patterns['HS'].append((window.index[0], window.index[-1]))\n",
        "\n",
        "        # Inverse Head and Shoulders\n",
        "        elif (e1 < e2) and (e3 < e1) and (e3 < e5) and \\\n",
        "            (abs(e1 - e5) <= 0.03*np.mean([e1,e5])) and \\\n",
        "            (abs(e2 - e4) <= 0.03*np.mean([e1,e5])):\n",
        "                patterns['IHS'].append((window.index[0], window.index[-1]))\n",
        "\n",
        "        # Broadening Top\n",
        "        elif (e1 > e2) and (e1 < e3) and (e3 < e5) and (e2 > e4):\n",
        "            patterns['BTOP'].append((window.index[0], window.index[-1]))\n",
        "\n",
        "        # Broadening Bottom\n",
        "        elif (e1 < e2) and (e1 > e3) and (e3 > e5) and (e2 < e4):\n",
        "            patterns['BBOT'].append((window.index[0], window.index[-1]))\n",
        "\n",
        "        # Triangle Top\n",
        "        elif (e1 > e2) and (e1 > e3) and (e3 > e5) and (e2 < e4):\n",
        "            patterns['TTOP'].append((window.index[0], window.index[-1]))\n",
        "\n",
        "        # Triangle Bottom\n",
        "        elif (e1 < e2) and (e1 < e3) and (e3 < e5) and (e2 > e4):\n",
        "            patterns['TBOT'].append((window.index[0], window.index[-1]))\n",
        "\n",
        "        # Rectangle Top\n",
        "        elif (e1 > e2) and (abs(e1-rtop_g1)/rtop_g1 < 0.0075) and \\\n",
        "            (abs(e3-rtop_g1)/rtop_g1 < 0.0075) and (abs(e5-rtop_g1)/rtop_g1 < 0.0075) and \\\n",
        "            (abs(e2-rtop_g2)/rtop_g2 < 0.0075) and (abs(e4-rtop_g2)/rtop_g2 < 0.0075) and \\\n",
        "            (min(e1, e3, e5) > max(e2, e4)):\n",
        "\n",
        "            patterns['RTOP'].append((window.index[0], window.index[-1]))\n",
        "\n",
        "        # Rectangle Bottom\n",
        "        elif (e1 < e2) and (abs(e1-rtop_g1)/rtop_g1 < 0.0075) and \\\n",
        "            (abs(e3-rtop_g1)/rtop_g1 < 0.0075) and (abs(e5-rtop_g1)/rtop_g1 < 0.0075) and \\\n",
        "            (abs(e2-rtop_g2)/rtop_g2 < 0.0075) and (abs(e4-rtop_g2)/rtop_g2 < 0.0075) and \\\n",
        "            (max(e1, e3, e5) > min(e2, e4)):\n",
        "            patterns['RBOT'].append((window.index[0], window.index[-1]))\n",
        "\n",
        "    return patterns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHqBmCAGaPn9"
      },
      "source": [
        "# Predictive Power of Technical Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNmsKmJaaPn9"
      },
      "outputs": [],
      "source": [
        "def compute_pattern_returns(prices, indentification_lag=4):\n",
        "    max_min, smooth_prices = find_max_min(prices)\n",
        "    patterns = find_patterns(max_min)\n",
        "    returns = (prices.pct_change(1)\n",
        "                          .shift(-1)\n",
        "                          .reset_index(drop=True))\n",
        "\n",
        "    demeaned_returns = (returns - returns.mean()) / returns.std()\n",
        "    pattern_mean_returns = pd.Series()\n",
        "    for name, start_end_day_nums in patterns.items():\n",
        "        if not isinstance(start_end_day_nums, list):\n",
        "            end_day_nums = [end_day_nums]\n",
        "        lagged_end_days = map(lambda x: x[1] + indentification_lag, start_end_day_nums)\n",
        "        pattern_mean_returns[name] = demeaned_returns.loc[lagged_end_days].mean()\n",
        "        #패턴 발생 x일 후 시점의 일일 정규화 수익률\n",
        "    return pattern_mean_returns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KfQ_nZ0aPn9"
      },
      "source": [
        "### Function to download daily stock data from yahoofinance api and save it to a csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWiNNYVsaPn-"
      },
      "outputs": [],
      "source": [
        "def download_csv_data(ticker, start_date, end_date, path):\n",
        "\n",
        "    df = fdr.DataReader(ticker, start_date, end_date)\n",
        "    df = df[['open', 'high', 'low', 'close', 'volume']]\n",
        "    df['date'] = df.index\n",
        "    df['dividend'] = 0  # dividend 열 추가\n",
        "    df['split'] = 1  # split 열 추가\n",
        "\n",
        "    # save data to csv for later ingestion\n",
        "    df.to_csv(path, header=True, index=True)\n",
        "\n",
        "    # plot the time series\n",
        "    df.close.plot(title='{} prices --- {}:{}'.format(ticker, start_date, end_date));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0jeEAMlaPn-"
      },
      "source": [
        "### Fetching S&P 500 stocks list from wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIN6pFhCT-Og"
      },
      "outputs": [],
      "source": [
        "import bs4 as bs\n",
        "import pickle\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def save_sp500_tickers():\n",
        "    resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
        "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
        "    table = soup.find('table', {'id': 'constituents'})\n",
        "    tickers = []\n",
        "\n",
        "    for row in table.find_all('tr')[1:]:\n",
        "        ticker = row.find_all('td')[0].text.strip('\\n')\n",
        "        tickers.append(ticker)\n",
        "\n",
        "    with open(\"sp500tickers.pickle\",\"wb\") as f:\n",
        "        pickle.dump(tickers,f)\n",
        "\n",
        "    return tickers\n",
        "\n",
        "tickers= save_sp500_tickers()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbUFSPAuaPn-",
        "outputId": "4883fecc-5594-41cb-b59d-52dcf388d74d"
      },
      "outputs": [],
      "source": [
        "tickers= [x for x in tickers if \".B\" not in x]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "UpiSDoeCaPn-"
      },
      "outputs": [],
      "source": [
        "text_start_date= '2008-01-01'#'2024-01-01'\n",
        "text_end_date= '2021-07-31'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "R9Bu7tFMaPn_",
        "outputId": "dcf97d92-5f43-454a-a92a-eeaa2c7de397"
      },
      "outputs": [],
      "source": [
        "bundle_name = 'SP500_traditional'\n",
        "tickers=tickers\n",
        "\n",
        "folder_path = bundle_name + '/daily/'\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "\n",
        "for ticker in tickers:\n",
        "    try:\n",
        "        download_csv_data(ticker=ticker,\n",
        "                          start_date=text_start_date,\n",
        "                          end_date=text_end_date,\n",
        "                          path=folder_path + ticker.strip(' .^')+'.csv')\n",
        "    except:\n",
        "        print(ticker+ \" failed\")\n",
        "\n",
        "    df=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "dn_TBr-cCe9y",
        "outputId": "049b50b5-7c9b-422d-89a1-c1240830a26f"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# DD 폴더 안의 모든 CSV 파일 경로를 찾습니다.\n",
        "data_folder = '/Data/BTC'\n",
        "csv_files = glob.glob(data_folder+'/*.csv')\n",
        "\n",
        "# 빈 리스트를 준비하여 각 데이터프레임을 추가할 예정입니다.\n",
        "dfs = []\n",
        "dfs_volumn = []\n",
        "dfs_rsi = []\n",
        "\n",
        "# 각 CSV 파일을 읽어서 'close' 컬럼만 선택하여 처리합니다.\n",
        "for file in csv_files:\n",
        "    # 파일을 읽고 'date' 컬럼을 인덱스로 설정합니다.\n",
        "    df = pd.read_csv(file, parse_dates=['Date'], index_col='Date')\n",
        "\n",
        "    # 'close' 컬럼만 선택하고 컬럼 이름을 파일명에서 'csv' 확장자를 제거한 값으로 설정합니다.\n",
        "    column_name = os.path.basename(file).replace('.csv', '')\n",
        "    df_close = df[['Close']].rename(columns={'Close': column_name})\n",
        "    df_volume = df[['Volume']].rename(columns={'Volume': column_name})\n",
        "    df_rsi = df[['RSI']].rename(columns={'RSI': column_name})\n",
        "\n",
        "    # 리스트에 데이터프레임을 추가합니다.\n",
        "    dfs.append(df_close)\n",
        "    dfs_volumn.append(df_volume)\n",
        "    dfs_rsi.append(df_rsi)\n",
        "\n",
        "\n",
        "# 모든 데이터프레임을 하나로 합칩니다.\n",
        "df_combined = pd.concat(dfs, axis=1)\n",
        "df_combined.index.name = 'index'\n",
        "df_combined_volumn = pd.concat(dfs_volumn, axis=1)\n",
        "df_combined_volumn.index.name = 'index'\n",
        "df_combined_rsi = pd.concat(dfs_rsi, axis=1)\n",
        "df_combined_rsi.index.name = 'index'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_combined.drop(df_combined.index[-1], inplace=True)\n",
        "df_combined_volumn.drop(df_combined_volumn.index[-1], inplace=True)\n",
        "df_combined_rsi.drop(df_combined_rsi.index[-1], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_combined = df_combined.astype(float)\n",
        "df_combined_volumn = df_combined_volumn.astype(float)\n",
        "df_combined_rsi = df_combined_rsi.astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "컬럼 개수: 418\n"
          ]
        }
      ],
      "source": [
        "df_combined.dropna(axis=1, inplace=True)\n",
        "df_combined_volumn.dropna(axis=1, inplace=True)\n",
        "df_combined_rsi.dropna(axis=1, inplace=True)\n",
        "\n",
        "print(f\"컬럼 개수: {df_combined.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "collapsed": true,
        "id": "DT1I1F-kBXEC"
      },
      "outputs": [],
      "source": [
        "pattern_returns = df_combined.apply(compute_pattern_returns)\n",
        "pattern_returns.rename(index={'BBOT': 'BBOT(P)', 'BTOP': 'BTOP(N)',\n",
        "                              'HS': 'HS(N)', 'IHS': 'IHS(P)', 'RTOP': 'RTOP(N)',\n",
        "                              'TBOT': 'TBOT(P)', 'TTOP': 'TTOP(N)', 'RBOT': 'RBOT(P)'}, inplace=True)\n",
        "new_order = ['IHS(P)', 'BBOT(P)', 'TBOT(P)', 'RBOT(P)', 'HS(N)', 'BTOP(N)', 'RTOP(N)', 'TTOP(N)']\n",
        "pattern_returns = pattern_returns.reindex(new_order)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxBXdJOpSx_r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "# 패턴을 종가 시계열 데이터로 변환하는 함수\n",
        "def convert_patterns_to_prices(df_combined, smoothing_combined):\n",
        "    all_patterns = defaultdict(list)\n",
        "\n",
        "    # 각 티커(컬럼)에 대해 for문 실행\n",
        "    for ticker in df_combined.columns:\n",
        "        max_min, _ = find_max_min(df_combined[ticker])\n",
        "        pattern_results = find_patterns(max_min)\n",
        "\n",
        "        for pattern_name, index_ranges in pattern_results.items():\n",
        "            for start_idx, end_idx in index_ranges:\n",
        "                # 인덱스를 날짜와 종가 데이터로 변환\n",
        "                time_series = smoothing_combined[ticker].iloc[start_idx:end_idx + 1]\n",
        "                time_series_volumn = df_combined_volumn[ticker].iloc[start_idx:end_idx + 1]\n",
        "                time_series_rsi = df_combined_rsi[ticker].iloc[start_idx:end_idx + 1]\n",
        "                all_patterns[pattern_name].append([time_series, time_series_volumn, time_series_rsi])\n",
        "\n",
        "    return all_patterns\n",
        "\n",
        "\n",
        "smoothing_dict = {}\n",
        "for ticker in df_combined.columns:\n",
        "    # Kernel Regression 적용\n",
        "    kr = KernelReg([df_combined[ticker].values], \n",
        "                   [np.linspace(1., len(df_combined), len(df_combined))], \n",
        "                   var_type='c', \n",
        "                   bw=np.full((1, 1), 0.8))\n",
        "    \n",
        "    f = kr.fit([np.linspace(1., len(df_combined), len(df_combined))])\n",
        "    smooth_prices = pd.Series(data=f[0], index=df_combined.index)  # 원래 인덱스로 설정\n",
        "    smoothing_dict[ticker] = smooth_prices  # 딕셔너리에 저장\n",
        "# 2️⃣ 한 번에 DataFrame으로 변환\n",
        "smoothing_combined = pd.DataFrame(smoothing_dict, index=df_combined.index).copy()\n",
        "\n",
        "# 패턴별 종가 시계열 변환 실행\n",
        "pattern_time_series = convert_patterns_to_prices(df_combined, smoothing_combined)\n",
        "\n",
        "# 출력 확인\n",
        "for pattern, series_list in pattern_time_series.items():\n",
        "    print(\"{}: {} occurences\".format(pattern, len(series_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-vkOMvaYJcTE"
      },
      "outputs": [],
      "source": [
        "!pip install tslearn\n",
        "!pip install fastdtw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. 각 timeseries마다 min-max 정규화\n",
        "2. interpolation\n",
        "3. close price 변수에 대해 패턴별 dba mean 계산\n",
        "4. 각 패턴 집합 속 dba mean과 close price의 path matrix 계산 후 volume, rsi에 매칭\n",
        "5. path matrix를 적용한 volumn, rsi의 mean 계산 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeVJq6YZaaOC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tslearn.barycenters import dtw_barycenter_averaging\n",
        "from collections import defaultdict\n",
        "from fastdtw import fastdtw\n",
        "\n",
        "# Min-Max 정규화 함수\n",
        "def min_max_scaling(series):\n",
        "    min_val = series.min()\n",
        "    max_val = series.max()\n",
        "    return (series - min_val) / (max_val - min_val) if max_val != min_val else series * 0  # 같은 값만 있으면 0으로 처리\n",
        "\n",
        "\n",
        "def interpolate_series(series_list, new_length=50):\n",
        "    \"\"\"\n",
        "    길이가 다른 시계열을 동일한 길이로 보간 (Interpolation)하는 함수.\n",
        "    \n",
        "    Args:\n",
        "        series_list (list of np.array): 원본 시계열 리스트\n",
        "    Returns:\n",
        "        np.array: 보간된 시계열 리스트\n",
        "    \"\"\"\n",
        "    if not series_list:\n",
        "        return None\n",
        "\n",
        "    interpolated_series = np.array([\n",
        "        np.interp(\n",
        "            np.linspace(0, len(series)-1, new_length),  # 새롭게 생성할 x값 (균등한 간격)\n",
        "            np.arange(len(series)),  # 원래 x값 (0, 1, 2, ...)\n",
        "            series  # 원래 y값\n",
        "        ) for series in series_list\n",
        "    ])\n",
        "    \n",
        "    return interpolated_series\n",
        "\n",
        "\n",
        "def remove_duplicate_indices(path):\n",
        "    seen = set()\n",
        "    result = []\n",
        "    \n",
        "    for item in path:\n",
        "        if item[1] not in seen:\n",
        "            result.append(item)\n",
        "            seen.add(item[1])\n",
        "    \n",
        "    return result\n",
        "\n",
        "# 시각화\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "centroid_dict = {}\n",
        "\n",
        "for pattern, series_list in pattern_time_series.items():\n",
        "    # Min-Max 정규화\n",
        "    normalized_series_list = [[min_max_scaling(ts) for ts in sublist] for sublist in series_list]\n",
        "    \n",
        "    # 1️⃣ **Interpolation 적용 (Close Price, Volume, RSI)**\n",
        "    close_prices = [ts[0] for ts in normalized_series_list]\n",
        "    volumes = [ts[1] for ts in normalized_series_list]\n",
        "    rsis = [ts[2] for ts in normalized_series_list]\n",
        "\n",
        "    interpolated_close_prices = interpolate_series(close_prices)\n",
        "    interpolated_volumes = interpolate_series(volumes)\n",
        "    interpolated_rsis = interpolate_series(rsis)\n",
        "    \n",
        "    # DBA 중심 시계열 계산\n",
        "    centroid = dtw_barycenter_averaging(interpolated_close_prices)\n",
        "\n",
        "    aligned_volumes = []\n",
        "    aligned_rsis = []\n",
        "    \n",
        "    for close_price, volume, rsi in zip(interpolated_close_prices, interpolated_volumes, interpolated_rsis):\n",
        "        _, path = fastdtw(close_price, centroid)\n",
        "        # 해당 시계열의 Volume과 RSI를 DTW path에 맞춰 재정렬. 그 전에 path를 치역을 일대일로 대응되게 \n",
        "        path = remove_duplicate_indices(path)\n",
        "        volume_warped = np.array([volume[i] for i, _ in path])\n",
        "        rsi_warped = np.array([rsi[i] for i, _ in path])\n",
        "        \n",
        "        aligned_volumes.append(volume_warped)\n",
        "        aligned_rsis.append(rsi_warped)\n",
        "    \n",
        "    # 3️⃣ Volume과 RSI의 최종 DBA 평균 계산\n",
        "    centroid_volume = np.mean(aligned_volumes, axis=0)\n",
        "    centroid_rsi = np.mean(aligned_rsis, axis=0)\n",
        "    \n",
        "    # Centroid 저장 (딕셔너리에 추가)\n",
        "    centroid_dict[pattern] = {'close': centroid.squeeze(), \n",
        "                              'volume': centroid_volume, \n",
        "                              'rsi': centroid_rsi}\n",
        "\n",
        "    # 그래프 생성\n",
        "    plt.figure(figsize=(8, 5))\n",
        "\n",
        "    # 각 개별 시계열을 점선으로 플로팅\n",
        "    for series in close_prices[:3]:\n",
        "        plt.plot(np.linspace(0, 1, len(series)), series.values, alpha=0.5, linestyle='dashed')\n",
        "\n",
        "    # Centroid 시계열 플로팅 (굵은 실선)\n",
        "    plt.plot(np.linspace(0, 1, len(centroid)), centroid, label=f'Centroid - {pattern}', linewidth=2, color='red')\n",
        "\n",
        "    # 그래프 타이틀 및 범례 추가\n",
        "    plt.title(f'Centroid Visualization for {pattern}')\n",
        "    plt.legend()\n",
        "\n",
        "    # 그래프 저장\n",
        "    plt.savefig(bundle_name + f'/centroid_{pattern}.png')\n",
        "\n",
        "    # 그래프 출력\n",
        "    plt.show()\n",
        "\n",
        "    # 그래프 닫기 (다음 패턴을 위해)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams.update(mpl.rcParamsDefault)\n",
        "\n",
        "def plot_close_only(centroid_dict):\n",
        "    selected_keys = ['IHS', 'HS', 'TBOT', 'TTOP', 'BBOT', 'BTOP']  # 선택 및 순서 지정\n",
        "\n",
        "    fig, axs = plt.subplots(1, len(selected_keys), figsize=(len(selected_keys) * 3, 2.3))\n",
        "    \n",
        "    if len(selected_keys) == 1:\n",
        "        axs = [axs]\n",
        "    \n",
        "    for i, name in enumerate(selected_keys):\n",
        "        data = centroid_dict[name]\n",
        "        close_series = data['close']\n",
        "        min_val, max_val = np.min(close_series), np.max(close_series)\n",
        "\n",
        "        if max_val - min_val == 0:\n",
        "            norm_series = np.zeros_like(close_series)\n",
        "        else:\n",
        "            norm_series = (close_series - min_val) / (max_val - min_val)\n",
        "\n",
        "        low, high = 0.1, 0.9  # 'c' 변수의 스케일링 범위\n",
        "        scaled_series = norm_series * (high - low) + low\n",
        "\n",
        "        axs[i].plot(scaled_series, color='orange', linewidth=4.0) \n",
        "        axs[i].set_title(name, fontsize=30)\n",
        "        axs[i].get_xaxis().set_visible(False)\n",
        "        axs[i].get_yaxis().set_visible(False)\n",
        "        axs[i].set_ylim(0, 1)\n",
        "        \n",
        "        for spine in axs[i].spines.values():\n",
        "            spine.set_linewidth(2)  # 테두리 굵게 설정\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 실행\n",
        "plot_close_only(centroid_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-lp39ocn1yN"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(bundle_name + f\"/centroids.csv\", 'wb') as f:\n",
        "    pickle.dump(centroid_dict, f)\n",
        "    \n",
        "\n",
        "with open(bundle_name + f\"/centroids.csv\", 'rb') as f:\n",
        "    loaded_dict = pickle.load(f)\n",
        "\n",
        "print(loaded_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "del loaded_dict['RTOP']\n",
        "del loaded_dict['RBOT']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 항목별로 stack\n",
        "result = np.array([\n",
        "    np.stack([v['close'], v['volume'], v['rsi']], axis=1)\n",
        "    for v in loaded_dict.values()\n",
        "])\n",
        "\n",
        "result.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6, 22, 3)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# 보간할 타겟 길이\n",
        "target_length = 22\n",
        "\n",
        "# 기존 인덱스 (0부터 49까지)\n",
        "x_original = np.linspace(0, 1, 50)\n",
        "\n",
        "# 새로운 인덱스 (0부터 1까지 22개 지점)\n",
        "x_new = np.linspace(0, 1, target_length)\n",
        "\n",
        "# 결과를 저장할 배열 초기화\n",
        "interpolated = np.zeros((6, target_length, 3))\n",
        "\n",
        "# 각 sample(0~5)에 대해 보간 수행\n",
        "for i in range(6):\n",
        "    for j in range(3):  # 각 채널(0~2)에 대해\n",
        "        f = interp1d(x_original, result[i, :, j], kind='linear')\n",
        "        interpolated[i, :, j] = f(x_new)\n",
        "\n",
        "print(interpolated.shape)  # (6, 22, 3)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ISIC",
      "language": "python",
      "name": "isic"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
